{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import layers \n",
    "from keras.layers import Input , Dense, Dropout, Flatten, Embedding, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import SGD, Adam, Adadelta, rmsprop\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2DTranspose, Conv2D\n",
    "from keras.utils.generic_utils import Progbar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "num_classes = 10\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_size):\n",
    "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
    "    # label drawn from P_c, to image space (..., 28, 28, 1)\n",
    "    cnn = Sequential()\n",
    "    \n",
    "    cnn.add(Dense(3 * 3 * 384, input_dim = latent_size, activation = 'relu'))\n",
    "    cnn.add(Reshape((3, 3, 384)))\n",
    "    \n",
    "    #up sample to (7, 7,....)\n",
    "    cnn.add(Conv2DTranspose(195, 5, strides= 1, padding= 'valid', activation = 'relu'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    \n",
    "    #upsample to (14, 14, ...)\n",
    "    cnn.add(Conv2DTranspose(96, 5, strides=2, padding = 'same', activation= 'relu'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    \n",
    "    #upsample to (28, 28, ......)\n",
    "    cnn.add(Conv2DTranspose(1, 5, strides= 2, padding = 'same', activation = 'tanh'))\n",
    "\n",
    "    # this is the z space commonly referred to in GAN papers\n",
    "    latent = Input(shape = (latent_size,))\n",
    "    \n",
    "    # this will be our label \n",
    "    image_class = Input(shape=(1,), dtype= 'int32')\n",
    "    \n",
    "    cls = Flatten()(Embedding(num_classes, latent_size)(image_class))\n",
    "    \n",
    "    # hadamard product between z-space and a class condtional embedding\n",
    "    h = layers.multiply([latent, cls])\n",
    "    fake_image = cnn(h)\n",
    "    \n",
    "    return Model([latent, image_class], fake_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    # build a relatively standard conv net, with LeakyReLUs as suggested in\n",
    "    # the reference paper\n",
    "    cnn = Sequential()\n",
    "    \n",
    "    cnn.add(Conv2D(32, 3, padding = 'same', strides = 2, input_shape = (28, 28, 1)))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    \n",
    "    cnn.add(Conv2D(64, 3, padding = 'same', strides= 1))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    \n",
    "    cnn.add(Conv2D(128, 3, padding = 'same', strides = 2))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    \n",
    "    cnn.add(Conv2D(256, 3, padding = 'same', strides= 2))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    \n",
    "    cnn.add(Flatten())\n",
    "    \n",
    "    image = Input(shape = (28, 28, 1))\n",
    "    features = cnn(image)\n",
    "    \n",
    "    fake = Dense(1, activation = 'sigmoid', name = 'generation')(features)\n",
    "    aux = Dense(num_classes, activation= 'softmax', name = 'auxiliary')(features)\n",
    "    return Model(image, [fake, aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_36 (InputLayer)           (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_17 (Sequential)      (None, 4096)         387840      input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "generation (Dense)              (None, 1)            4097        sequential_17[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary (Dense)               (None, 10)           40970       sequential_17[1][0]              \n",
      "==================================================================================================\n",
      "Total params: 432,907\n",
      "Trainable params: 432,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "combined model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_40 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_23 (Model)                (None, 28, 28, 1)    2693912     input_39[0][0]                   \n",
      "                                                                 input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_22 (Model)                [(None, 1), (None, 1 432907      model_23[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,126,819\n",
      "Trainable params: 2,693,330\n",
      "Non-trainable params: 433,489\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    epochs = 100\n",
    "    batch_size = 100\n",
    "    latent_size = 100\n",
    "    \n",
    "    adam_lr = 0.0002\n",
    "    adam_beta_1 = 0.5\n",
    "    \n",
    "    # build the discriminator \n",
    "    print('Discriminator model:')\n",
    "    discriminator = build_discriminator()\n",
    "    discriminator.compile(optimizer=Adam(lr = adam_lr, beta_1 = adam_beta_1), \n",
    "                          loss = ['binary_crossentropy', 'sparse_categorical_crossentropy'])\n",
    "    discriminator.summary()\n",
    "    \n",
    "    #build the generator \n",
    "    generator = build_generator(latent_size)\n",
    "    latent = Input(shape = (latent_size,))\n",
    "    image_class = Input(shape =(1,), dtype ='int32')\n",
    "    \n",
    "    # get the fake image \n",
    "    fake = generator([latent, image_class])\n",
    "    \n",
    "    # we only want to be able to train generation for the combined model\n",
    "    discriminator.trainable = False\n",
    "    fake, aux = discriminator(fake)\n",
    "    combined = Model([latent, image_class], [fake, aux])\n",
    "    \n",
    "    print('combined model:')\n",
    "    combined.compile(optimizer = Adam(lr = adam_lr, beta_1 = adam_beta_1),\n",
    "                    loss = ['binary_crossentropy', 'sparse_categorical_crossentropy'])\n",
    "    combined.summary()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-30-6c903ab8ce37>, line 69)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-30-6c903ab8ce37>\"\u001b[1;36m, line \u001b[1;32m69\u001b[0m\n\u001b[1;33m    progress_bar.update(index + 1)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# get our mnist data, and force it to be of shape (...., 28, 28, 1) with \n",
    "# range [-1, 1]\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "x_train = np.expand_dims(x_train, axis = -1)\n",
    "    \n",
    "x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n",
    "x_test = np.expand_dims(x_test, axis = -1)\n",
    "    \n",
    "num_train, num_test = x_train.shape[0], x_test.shape[0]\n",
    "    \n",
    "train_history = defaultdict(list)\n",
    "test_history = defaultdict(list)\n",
    "    \n",
    "for epoch in range(1, epochs + 1):\n",
    "    print('Epoch {}/{}'.format(epoch, epochs))\n",
    "        \n",
    "    num_batches = int(x_train.shape[0] / batch_size)\n",
    "    progress_bar = Progbar(target = num_batches)\n",
    "    disc_sample_weights = [np.ones(2 * batch_size), np.concatenate((np.ones(batch_size) * 2, np.zeros(batch_size)))]\n",
    "    epoch_gen_loss = []\n",
    "    epoch_disc_loss = []\n",
    "        \n",
    "        \n",
    "    for index in range(batch_size):\n",
    "        noise = np.random.uniform(-1,1, (batch_size, latent_size))\n",
    "            \n",
    "        # get the batch of real image\n",
    "        image_batch = x_train[index * batch_size:(index + 1) * batch_size]\n",
    "        label_batch = x_trian[index * batch_size:(index + 1) * batch_size]\n",
    "        # sample some labels from p_c\n",
    "        sampled_labels = np.random.randint(0, num_classes, batch_size)\n",
    "\n",
    "        # generate a batch of fake images, using the generated labels as a\n",
    "        # conditioner. We reshape the sampled labels to be\n",
    "        # (batch_size, 1) so that we can feed them into the embedding\n",
    "        # layer as a length one sequence\n",
    "        generated_images = generator.predict(\n",
    "                [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n",
    "\n",
    "        x = np.concatenate((image_batch, generated_images))\n",
    "\n",
    "        # use one-sided soft real/fake labels\n",
    "        # Salimans et al., 2016\n",
    "        # https://arxiv.org/pdf/1606.03498.pdf (Section 3.4)\n",
    "        soft_zero, soft_one = 0, 0.95\n",
    "        y = np.array([soft_one] * batch_size + [soft_zero] * batch_size)\n",
    "        aux_y = np.concatenate((label_batch, sampled_labels), axis=0)\n",
    "\n",
    "        # see if the discriminator can figure itself out...\n",
    "        epoch_disc_loss.append(discriminator.train_on_batch(\n",
    "                x, [y, aux_y], sample_weight=disc_sample_weight))\n",
    "\n",
    "        # make new noise. we generate 2 * batch size here such that we have\n",
    "        # the generator optimize over an identical number of images as the\n",
    "        # discriminator\n",
    "        noise = np.random.uniform(-1, 1, (2 * batch_size, latent_size))\n",
    "        sampled_labels = np.random.randint(0, num_classes, 2 * batch_size)\n",
    "\n",
    "        # we want to train the generator to trick the discriminator\n",
    "        # For the generator, we want all the {fake, not-fake} labels to say\n",
    "        # not-fake\n",
    "        trick = np.ones(2 * batch_size) * soft_one\n",
    "\n",
    "        epoch_gen_loss.append(combined.train_on_batch(\n",
    "                [noise, sampled_labels.reshape((-1, 1))],\n",
    "                [trick, sampled_labels]))\n",
    "\n",
    "            progress_bar.update(index + 1)\n",
    "\n",
    "        print('Testing for epoch {}:'.format(epoch))\n",
    "\n",
    "        # evaluate the testing loss here\n",
    "\n",
    "        # generate a new batch of noise\n",
    "        noise = np.random.uniform(-1, 1, (num_test, latent_size))\n",
    "\n",
    "        # sample some labels from p_c and generate images from them\n",
    "        sampled_labels = np.random.randint(0, num_classes, num_test)\n",
    "        generated_images = generator.predict(\n",
    "            [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n",
    "\n",
    "        x = np.concatenate((x_test, generated_images))\n",
    "        y = np.array([1] * num_test + [0] * num_test)\n",
    "        aux_y = np.concatenate((y_test, sampled_labels), axis=0)\n",
    "\n",
    "        # see if the discriminator can figure itself out...\n",
    "        discriminator_test_loss = discriminator.evaluate(\n",
    "            x, [y, aux_y], verbose=False)\n",
    "\n",
    "        discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n",
    "\n",
    "        # make new noise\n",
    "        noise = np.random.uniform(-1, 1, (2 * num_test, latent_size))\n",
    "        sampled_labels = np.random.randint(0, num_classes, 2 * num_test)\n",
    "\n",
    "        trick = np.ones(2 * num_test)\n",
    "\n",
    "        generator_test_loss = combined.evaluate(\n",
    "            [noise, sampled_labels.reshape((-1, 1))],\n",
    "            [trick, sampled_labels], verbose=False)\n",
    "\n",
    "        generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n",
    "\n",
    "        # generate an epoch report on performance\n",
    "        train_history['generator'].append(generator_train_loss)\n",
    "        train_history['discriminator'].append(discriminator_train_loss)\n",
    "\n",
    "        test_history['generator'].append(generator_test_loss)\n",
    "        test_history['discriminator'].append(discriminator_test_loss)\n",
    "\n",
    "        print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
    "            'component', *discriminator.metrics_names))\n",
    "        print('-' * 65)\n",
    "\n",
    "        ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.4f} | {3:<5.4f}'\n",
    "        print(ROW_FMT.format('generator (train)',\n",
    "                             *train_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('generator (test)',\n",
    "                             *test_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (train)',\n",
    "                             *train_history['discriminator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (test)',\n",
    "                             *test_history['discriminator'][-1]))\n",
    "\n",
    "        # save weights every epoch\n",
    "        generator.save_weights(\n",
    "            'params_generator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "        discriminator.save_weights(\n",
    "            'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "\n",
    "        # generate some digits to display\n",
    "        num_rows = 40\n",
    "        noise = np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)),\n",
    "                        (num_classes, 1))\n",
    "\n",
    "        sampled_labels = np.array([\n",
    "            [i] * num_rows for i in range(num_classes)\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "        # get a batch to display\n",
    "        generated_images = generator.predict(\n",
    "            [noise, sampled_labels], verbose=0)\n",
    "\n",
    "        # prepare real images sorted by class label\n",
    "        real_labels = y_train[(epoch - 1) * num_rows * num_classes:\n",
    "                              epoch * num_rows * num_classes]\n",
    "        indices = np.argsort(real_labels, axis=0)\n",
    "        real_images = x_train[(epoch - 1) * num_rows * num_classes:\n",
    "                              epoch * num_rows * num_classes][indices]\n",
    "\n",
    "        # display generated images, white separator, real images\n",
    "        img = np.concatenate(\n",
    "            (generated_images,\n",
    "             np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0),\n",
    "             real_images))\n",
    "\n",
    "        # arrange them into a grid\n",
    "        img = (np.concatenate([r.reshape(-1, 28)\n",
    "                               for r in np.split(img, 2 * num_classes + 1)\n",
    "                               ], axis=-1) * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "        Image.fromarray(img).save(\n",
    "            'plot_epoch_{0:03d}_generated.png'.format(epoch))\n",
    "\n",
    "    with open('acgan-history.pkl', 'wb') as f:\n",
    "        pickle.dump({'train': train_history, 'test': test_history}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
